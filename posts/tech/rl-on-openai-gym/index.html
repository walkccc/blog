<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Reinforcement Learning on OpenAI Gym &#183; Peng-Yu's Blog</title>
<meta name=title content="Reinforcement Learning on OpenAI Gym &#183; Peng-Yu's Blog"><meta name=keywords content="Reinforcement Learning,Python,"><link rel=canonical href=https://blog.pengyuc.com/posts/tech/rl-on-openai-gym/><link type=text/css rel=stylesheet href=/css/main.bundle.min.36c3cd7950e4533fa7da3150d972e3edf34d07f83c0264ff04cad0969dfdb3b8a7065b0ed6c730c6d34a7bad516cfc6f6a5917ab1fdb10b25f481f8a17b54c16.css integrity="sha512-NsPNeVDkUz+n2jFQ2XLj7fNNB/g8AmT/BMrQlp39s7inBlsO1scwxtNKe61RbPxvalkXqx/bELJfSB+KF7VMFg=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.3ad74b33440334ca29f8801ec6dac1f314951cf580e503d49631816b48d1f28d03543275f92b651565b48045b90731f47595b1396214b8b6aa11c6990f867f7e.js integrity="sha512-OtdLM0QDNMop+IAextrB8xSVHPWA5QPUljGBa0jR8o0DVDJ1+StlFWW0gEW5BzH0dZWxOWIUuLaqEcaZD4Z/fg==" data-copy=Copy data-copied=Copied></script><script src=/lib/zoom/zoom.min.f592a181a15d2a5b042daa7f746c3721acf9063f8b6acd175d989129865a37d400ae0e85b640f9ad42cd98d1f8ad30931718cf8811abdcc5fcb264400d1a2b0c.js integrity="sha512-9ZKhgaFdKlsELap/dGw3Iaz5Bj+Las0XXZiRKYZaN9QArg6FtkD5rULNmNH4rTCTFxjPiBGr3MX8smRADRorDA=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://blog.pengyuc.com/posts/tech/rl-on-openai-gym/"><meta property="og:site_name" content="Peng-Yu's Blog"><meta property="og:title" content="Reinforcement Learning on OpenAI Gym"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-12-06T00:00:00+00:00"><meta property="article:modified_time" content="2018-12-06T00:00:00+00:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Python"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning on OpenAI Gym"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Reinforcement Learning on OpenAI Gym","headline":"Reinforcement Learning on OpenAI Gym","inLanguage":"en","url":"https:\/\/blog.pengyuc.com\/posts\/tech\/rl-on-openai-gym\/","author":{"@type":"Person","name":"Peng-Yu Chen"},"copyrightYear":"2018","dateCreated":"2018-12-06T00:00:00\u002b00:00","datePublished":"2018-12-06T00:00:00\u002b00:00","dateModified":"2018-12-06T00:00:00\u002b00:00","keywords":["Reinforcement Learning","Python"],"mainEntityOfPage":"true","wordCount":"1587"}]</script><meta name=author content="Peng-Yu Chen"><link href=https://pengyuc.com rel=me><link href=https://github.com/walkccc rel=me><link href=https://linkedin.com/in/pengyuc rel=me><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><link type=text/css rel=stylesheet href=/lib/katex/katex.min.7e7e35e3ef02b7b437449a44ca3fac62ec1ed39cb8312b680a00fe8ac60badc95b063b694636b8440856f7f5e8c2cc9e6b0efb581179b2656c7e1e97558c7096.css integrity="sha512-fn414+8Ct7Q3RJpEyj+sYuwe05y4MStoCgD+isYLrclbBjtpRja4RAhW9/Xowsyeaw77WBF5smVsfh6XVYxwlg=="><script defer src=/lib/katex/katex.min.cadd45c1af1f44bdaf196dc9b104f1daeb29043f0dc59155ffe22847510a04390a0b7a859400d420a626204f7fc5ddb07c19311de1c66b25e19c2559d3e126a8.js integrity="sha512-yt1Fwa8fRL2vGW3JsQTx2uspBD8NxZFV/+IoR1EKBDkKC3qFlADUIKYmIE9/xd2wfBkxHeHGayXhnCVZ0+EmqA=="></script><script defer src=/lib/katex/auto-render.min.e9b2833d28623d18c071d78ef13e9c79d695122d296af3dbcee7bf1bf6518b0565bab59939267fbc8f5faf696193c20f5caef3e7501969cfb306f6738032730d.js integrity="sha512-6bKDPShiPRjAcdeO8T6cedaVEi0pavPbzue/G/ZRiwVlurWZOSZ/vI9fr2lhk8IPXK7z51AZac+zBvZzgDJzDQ==" onload=renderMathInElement(document.body)></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Peng-Yu&rsquo;s Blog</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Posts>Posts</p></a><a href=https://pengyuc.com target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Links
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=https://walkccc.me/LeetCode/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>LeetCode Solutions</p></a><a href=https://www.pokemontcgp.ai target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>PokémonTCGP.ai</p></a></div></div></div></div><a href=https://github.com/walkccc target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Posts>Posts</p></a></li><li class=mt-1><a href=https://pengyuc.com target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Links</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=https://walkccc.me/LeetCode/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>LeetCode Solutions</p></a></li><li class=mt-1><a href=https://www.pokemontcgp.ai target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>PokémonTCGP.ai</p></a></li><li class=mb-2></li><li class=mt-1><a href=https://github.com/walkccc target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul><hr><ul class="flex mt-4 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li class=mb-1><a href=/tags/ class="flex items-center"><p class="text-sm font-sm text-gray-500 hover:text-gray-900" title=Tags>Tags</p></a></li><li class=mb-1><a href=/categories/ class="flex items-center"><p class="text-sm font-sm text-gray-500 hover:text-gray-900" title=Categories>Categories</p></a></li></ul></div></label></div></div><div class="main-menu flex pb-3 flex-col items-end justify-between md:justify-start space-x-3"><div class="hidden md:flex items-center space-x-5"><a href=/tags/ class="flex items-center"><p class="text-xs font-light text-gray-500 hover:text-gray-900" title=Tags>Tags</p></a><a href=/categories/ class="flex items-center"><p class="text-xs font-light text-gray-500 hover:text-gray-900" title=Categories>Categories</p></a></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class=hidden><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/>Peng-Yu's Blog</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/>Posts</a><span class="px-1 text-primary-500">/</span></li><li class=hidden><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/tech/rl-on-openai-gym/>Reinforcement Learning on OpenAI Gym</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Reinforcement Learning on OpenAI Gym</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2018-12-06T00:00:00+00:00>6 December 2018</time><span class="px-2 text-primary-500">&#183;</span><span>1587 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">8 mins</span><span class="px-2 text-primary-500">&#183;</span>
<script type=text/javascript src=/js/zen-mode.min.eea5245cf9244ecbdf2c150d1c8833226c1541cadf6e98f63a7c9192b1a3676df2c3ec603b14f4cfaaa53971fd9d8955640c0f405bf3de2b43ee7a5fb29ae721.js integrity="sha512-7qUkXPkkTsvfLBUNHIgzImwVQcrfbpj2OnyRkrGjZ23yw+xgOxT0z6qlOXH9nYlVZAwPQFvz3itD7npfsprnIQ=="></script><span class=mb-[2px]><span id=zen-mode-button class="text-lg hover:text-primary-500" title="Enable zen mode" data-title-i18n-disable="Enable zen mode" data-title-i18n-enable="Disable zen mode"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 50 50" width="50" height="50"><path fill="currentcolor" d="M12.980469 4C9.1204688 4 5.9804688 7.14 5.9804688 11L6 26H9.9804688V11c0-1.65 1.3400002-3 3.0000002-3H40.019531c1.66.0 3 1.35 3 3V39c0 1.65-1.34 3-3 3H29c0 1.54-.579062 2.94-1.539062 4H40.019531c3.86.0 7-3.14 7-7V11c0-3.86-3.14-7-7-7H12.980469zM7 28c-2.206.0-4 1.794-4 4V42c0 2.206 1.794 4 4 4H23c2.206.0 4-1.794 4-4V32c0-2.206-1.794-4-4-4H7zm0 4H23L23.001953 42H7V32z"/></svg></span></span></span></span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/reinforcement-learning/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Reinforcement Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/python/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Python</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Peng-Yu Chen" src=/img/blowfish_logo_hu_e74a130226122ae3.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Peng-Yu Chen</div><div class="text-sm text-neutral-700 dark:text-neutral-400">A little bit about you</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=mailto:me@pengyuc.com target=_blank aria-label=Email rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://pengyuc.com target=_blank aria-label=Link rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/walkccc target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://linkedin.com/in/pengyuc target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#reinforcement-learning-介紹>Reinforcement Learning 介紹</a></li></ul><ul><li><a href=#random-action>Random Action</a></li><li><a href=#hand-made-policy>Hand-Made Policy</a></li><li><a href=#q-learning-with-q-table>Q-Learning with Q Table</a></li><li><a href=#deep-q-learning>Deep Q-Learning</a><ul><li><a href=#deep-q-learning-implementation>Deep Q-Learning Implementation</a><ul><li><a href=#step-1-建立-network>Step 1: 建立 Network</a></li><li><a href=#step-2-建立-deep-q-network>Step 2: 建立 Deep Q-Network</a></li><li><a href=#step-3-訓練>Step 3: 訓練</a></li></ul></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#reinforcement-learning-介紹>Reinforcement Learning 介紹</a></li></ul><ul><li><a href=#random-action>Random Action</a></li><li><a href=#hand-made-policy>Hand-Made Policy</a></li><li><a href=#q-learning-with-q-table>Q-Learning with Q Table</a></li><li><a href=#deep-q-learning>Deep Q-Learning</a><ul><li><a href=#deep-q-learning-implementation>Deep Q-Learning Implementation</a><ul><li><a href=#step-1-建立-network>Step 1: 建立 Network</a></li><li><a href=#step-2-建立-deep-q-network>Step 2: 建立 Deep Q-Network</a></li><li><a href=#step-3-訓練>Step 3: 訓練</a></li></ul></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})(),function(){var t,e=$("#TableOfContents");if(e.length>0){t=$(window);function n(){var s,o=t.scrollTop(),i=$(".anchor"),n="";if(i.each(function(e,t){t=$(t),t.offset().top-$(window).height()/3<=o&&(n=decodeURIComponent(t.attr("id")))}),s=e.find("a.active"),s.length==1&&s.eq(0).attr("href")=="#"+n)return!0;s.each(function(e,t){$(t).removeClass("active").siblings("ul").hide()}),e.find('a[href="#'+n+'"]').addClass("active"),e.find('a[href="#'+n+'"]').parentsUntil("#TableOfContents").each(function(e,t){$(t).children("a").parents("ul").show()})}t.on("scroll",n),$(document).ready(function(){e.find("a").parent("li").find("ul").hide(),n()})}}()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><h1 class="relative group">Reinforcement Learning: OpenAI Gym<div id=reinforcement-learning-openai-gym class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#reinforcement-learning-openai-gym aria-label=Anchor>#</a></span></h1><p>強化學習（Reinforcement Learning，以下簡稱 RL）有別與一般監督式學習（Supervised
Learning）與要 end-to-end 的資料訓練。</p><h2 class="relative group">Reinforcement Learning 介紹<div id=reinforcement-learning-%E4%BB%8B%E7%B4%B9 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#reinforcement-learning-%E4%BB%8B%E7%B4%B9 aria-label=Anchor>#</a></span></h2><blockquote><p>Agent 在某個 state 做了一個 action，而移到下一個 state，環境此時給了 agent 一
個 reward，而 agent 則根據這個 reward 調整他的腳步。</p></blockquote><p>聽起來有點抽象？比喻來說：</p><p>你是 agent，而環境是家（包括：爸爸、媽媽等），action 是你的行為舉止，state 是你
所在的地點，reward 是媽媽給你的獎勵（可能是負的，即：負回饋）。</p><ul><li>若你（agent）在客廳（state），打破了一個杯子（action），而媽媽斥責你（reward）
，你就會透過這次的不小心，去避免下次在客廳時不要再打破杯子。</li><li>相反地，若你（agent）在廚房（state），洗了碗筷（action），而媽媽說你很棒
（reward），你就會知道，洗碗可以幫忙分擔家事並得到正回饋，所以以後一樣到了廚房
看到有碗筷時，就會增加想洗碗的機率。</li></ul><hr><p>由上可知，RL 是 ML 家族中的一員，而我個人覺得他更有 AI 的感覺，他是一種目標導向
（goal-oriented）的學習方法，透過 agent 與環境間的互動獲得更種獎勵或懲罰，學會如
何做「最好的」決策（policy）。</p><p>整個決策決定過程可由以下 5 個要素組成：</p><ol><li>Agent：與 environment 互動（action）。</li><li>Action：agent 藉由自己的 policy 所做的動作。</li><li>Environment：agent 的行動範圍，根據 agent 的 action 給予不同的 reward。</li><li>State：agent 在特定時間處的狀態。</li><li>Reward：environment 根據 agent 的 action 給的獎勵或懲罰。</li></ol><h1 class="relative group">Algorithm Implementation<div id=algorithm-implementation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#algorithm-implementation aria-label=Anchor>#</a></span></h1><p>我們使用 OpenAI Gym 當中的 <code>CartPole-v0</code> 來實作 RL 演算法，OpenAL Gym 提供了各種
不同的 environment 來做 RL 的訓練。</p><p>為了避免跨平台上安裝的問題，我使用 Google Colab 的雲端平台來
<a href=https://colab.research.google.com/drive/1nellAKykrJOwySQZqZ8eagTmkR0PSXEE target=_blank>demo</a>。</p><p><a href=https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py target=_blank>CartPole source code</a></p><h2 class="relative group">Random Action<div id=random-action class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#random-action aria-label=Anchor>#</a></span></h2><p>先從最簡單的例子來了解相關變數，無論 environment 如何，隨機挑選一個可行的
action，即：隨機決定左移或右移。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>Random Action
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>env</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>make</span><span class=p>(</span><span class=s1>&#39;CartPole-v0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># try 30 episodes</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>30</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>obs</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=n>rewards</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># try 50 actions for each episode</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>50</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>action</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=o>.</span><span class=n>sample</span><span class=p>()</span>  <span class=c1># randomly choose 0 (left) or 1 (right)</span>
</span></span><span class=line><span class=cl>    <span class=n>obs</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards</span> <span class=o>+=</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>done</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;episode </span><span class=si>{}</span><span class=s1>: Episode finished after </span><span class=si>{}</span><span class=s1> timesteps, total rewards </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span>
</span></span><span class=line><span class=cl>          <span class=n>i</span><span class=p>,</span> <span class=n>t</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>rewards</span><span class=p>))</span>
</span></span><span class=line><span class=cl>      <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ipythondisplay</span><span class=o>.</span><span class=n>clear_output</span><span class=p>(</span><span class=n>wait</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>env</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span></code></pre></div><p>因為 agent 在這個 random choosing action 時，並沒有任何學習，所以 reward 普遍不
高。</p><h2 class="relative group">Hand-Made Policy<div id=hand-made-policy class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#hand-made-policy aria-label=Anchor>#</a></span></h2><p>再來我們引進一個簡單的 policy：</p><ul><li>如果柱子向左傾（角度 \(&lt; 0\)），則小車左移以維持平衡。</li><li>如果柱子向左傾（角度 \(\ge 0\)），則小車右移以維持平衡。</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>Hand-Made Policy
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define policy</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>choose_action</span><span class=p>(</span><span class=n>obs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>pos</span><span class=p>,</span> <span class=n>v</span><span class=p>,</span> <span class=n>ang</span><span class=p>,</span> <span class=n>rot</span> <span class=o>=</span> <span class=n>obs</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>0</span> <span class=k>if</span> <span class=n>ang</span> <span class=o>&lt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=mi>1</span>    <span class=c1># 0: left, 1: right</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>env</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>make</span><span class=p>(</span><span class=s1>&#39;CartPole-v0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>50</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>obs</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=n>rewards</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>250</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>action</span> <span class=o>=</span> <span class=n>choose_action</span><span class=p>(</span><span class=n>obs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>obs</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards</span> <span class=o>+=</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>done</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;episode </span><span class=si>{}</span><span class=s1>: Episode finished after </span><span class=si>{}</span><span class=s1> timesteps, total rewards </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span>
</span></span><span class=line><span class=cl>          <span class=n>i</span><span class=p>,</span> <span class=n>t</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>rewards</span><span class=p>))</span>
</span></span><span class=line><span class=cl>      <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>env</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span></code></pre></div><h2 class="relative group">Q-Learning with Q Table<div id=q-learning-with-q-table class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#q-learning-with-q-table aria-label=Anchor>#</a></span></h2><p>$$Q ^ { * } ( s , a ) = \sum _ { s ^ { \prime } } T \left( s , a , s ^ { \prime } \right) \left( R \left( s , a , s ^ { \prime } \right) + \gamma \max _ { a ^ { \prime } } Q ^ { * } \left( s ^ { \prime } , a ^ { \prime } \right) \right)$$</p><p>其中：</p><ul><li>\(T\)：transition function，\(0 \le T \le 1\) 表發生機率</li><li>\(R\)：reward function</li><li>\(\gamma\)：discount factor，通常會是一個 \(&lt; 1\) 的值，可能是
\(0.9\)、\(0.8\) 之類，代表的是對未來 reward 的重視程度。</li></ul><p>所以我們的目的是：</p><p>$$\arg\max_a Q^*(s, a).$$</p><p>agent 透過一次次跟 environment 互動（a）獲得的 reward 來學習 Q function：</p><p>$$
Q(s_t, a_t) = (1 - \alpha) \cdot Q(s_{t - 1}, a_{t - 1}) + \alpha \cdot (r_t + \gamma \max Q(s_{t + 1}, a))
$$</p><p>其中，</p><ul><li>\(t\)：不同的時間點</li><li>\(\alpha\)：learning rate</li></ul><p>pseudo code 如下：</p><pre tabindex=0><code>Initialize Q(s, a) randomly
for each episode
  Initialize s

  for each step of episode
      Choose a from s using policy derived from Q (e.g., ε-greedy)
      Take action a, observe r, s&#39;
      Q(s, a) ← Q(s, a) + α[r + γ max_a&#39; Q(s&#39;, a&#39;) - Q(s, a)]
      s ← s&#39;
  until s is terminal
</code></pre><p>\(\epsilon\)-greedy 是一種在 exploration 和 exploitation 間取得平衡的方法。</p><ul><li>exploration 嘗試不同 action</li><li>exploitation 沿用現有 policy</li></ul><p>方法很簡單：</p><ul><li>\(\epsilon\) 時間，agent 嘗試新 action</li><li>\((1 - \epsilon)\) 時間，agent 沿用現有 policy</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>Q-Learning
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>choose_action</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>QTable</span><span class=p>,</span> <span class=n>action_space</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random_sample</span><span class=p>()</span> <span class=o>&lt;</span> <span class=n>epsilon</span><span class=p>:</span>    <span class=c1># P(randomly choose action) = ε</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>action_space</span><span class=o>.</span><span class=n>sample</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=c1># choose the action that maximize QTable[state]</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>QTable</span><span class=p>[</span><span class=n>state</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_state</span><span class=p>(</span><span class=n>obs</span><span class=p>,</span> <span class=n>n_buckets</span><span class=p>,</span> <span class=n>bds</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>state</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>obs</span><span class=p>)</span>             <span class=c1># state = [0, 0, 0, 0]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>s</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>obs</span><span class=p>):</span>        <span class=c1># each feature has different distribution</span>
</span></span><span class=line><span class=cl>    <span class=n>l</span><span class=p>,</span> <span class=n>u</span> <span class=o>=</span> <span class=n>bds</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>bds</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>    <span class=c1># each feature&#39;s lowerbound &amp; upperbound</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>s</span> <span class=o>&lt;=</span> <span class=n>bds</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>]:</span>             <span class=c1># lower than lowerbound</span>
</span></span><span class=line><span class=cl>      <span class=n>state</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>s</span> <span class=o>&gt;=</span> <span class=n>bds</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>]:</span>           <span class=c1># higher than upperbound</span>
</span></span><span class=line><span class=cl>      <span class=n>state</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>n_buckets</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>state</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(((</span><span class=n>s</span> <span class=o>-</span> <span class=n>l</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>u</span> <span class=o>-</span> <span class=n>l</span><span class=p>))</span> <span class=o>*</span> <span class=n>n_buckets</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nb>tuple</span><span class=p>(</span><span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>env</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>make</span><span class=p>(</span><span class=s1>&#39;CartPole-v0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Prepare Q table</span>
</span></span><span class=line><span class=cl><span class=c1># Each feature&#39;s n_bucket</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;1&#39; represents that any value belongs to the same state, i.e., the feature is not important</span>
</span></span><span class=line><span class=cl><span class=n>n_buckets</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>n_actions</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=o>.</span><span class=n>n</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># bounds of state</span>
</span></span><span class=line><span class=cl><span class=n>bds</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>env</span><span class=o>.</span><span class=n>observation_space</span><span class=o>.</span><span class=n>low</span><span class=p>,</span> <span class=n>env</span><span class=o>.</span><span class=n>observation_space</span><span class=o>.</span><span class=n>high</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>bds</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>bds</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>radians</span><span class=p>(</span><span class=mi>50</span><span class=p>),</span> <span class=n>math</span><span class=o>.</span><span class=n>radians</span><span class=p>(</span><span class=mi>50</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Q Table, each (s, a) pair stores one value</span>
</span></span><span class=line><span class=cl><span class=n>QTable</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n_buckets</span> <span class=o>+</span> <span class=p>(</span><span class=n>n_actions</span><span class=p>,))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># epsilon-greedy, decrease by time</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>epsilons</span><span class=p>(</span><span class=n>i</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nb>max</span><span class=p>(</span><span class=mf>0.01</span><span class=p>,</span> <span class=nb>min</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mf>1.0</span> <span class=o>-</span> <span class=n>math</span><span class=o>.</span><span class=n>log10</span><span class=p>((</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=mi>25</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># learning rate, decrease by time</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>lrs</span><span class=p>(</span><span class=n>i</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nb>max</span><span class=p>(</span><span class=mf>0.01</span><span class=p>,</span> <span class=nb>min</span><span class=p>(</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>1.0</span> <span class=o>-</span> <span class=n>math</span><span class=o>.</span><span class=n>log10</span><span class=p>((</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=mi>25</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>gamma</span> <span class=o>=</span> <span class=mf>0.99</span>  <span class=c1># reward discount factor</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Q-Learning</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>200</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>obs</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=n>rewards</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>  <span class=c1># convert continuous value -&gt; discrete value</span>
</span></span><span class=line><span class=cl>  <span class=n>state</span> <span class=o>=</span> <span class=n>get_state</span><span class=p>(</span><span class=n>obs</span><span class=p>,</span> <span class=n>n_buckets</span><span class=p>,</span> <span class=n>bds</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>epsilon</span> <span class=o>=</span> <span class=n>epsilons</span><span class=p>(</span><span class=n>i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>lr</span> <span class=o>=</span> <span class=n>lrs</span><span class=p>(</span><span class=n>i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>250</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>action</span> <span class=o>=</span> <span class=n>choose_action</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>QTable</span><span class=p>,</span> <span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>obs</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards</span> <span class=o>+=</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>next_state</span> <span class=o>=</span> <span class=n>get_state</span><span class=p>(</span><span class=n>obs</span><span class=p>,</span> <span class=n>n_buckets</span><span class=p>,</span> <span class=n>bds</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># update Q Table</span>
</span></span><span class=line><span class=cl>    <span class=n>q_next_max</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>amax</span><span class=p>(</span><span class=n>QTable</span><span class=p>[</span><span class=n>next_state</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>QTable</span><span class=p>[</span><span class=n>state</span> <span class=o>+</span> <span class=p>(</span><span class=n>action</span><span class=p>,)]</span> <span class=o>+=</span> <span class=n>lr</span> <span class=o>*</span> <span class=p>(</span><span class=n>reward</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>                                       <span class=n>q_next_max</span> <span class=o>-</span> <span class=n>QTable</span><span class=p>[</span><span class=n>state</span> <span class=o>+</span> <span class=p>(</span><span class=n>action</span><span class=p>,)])</span>  <span class=c1># Formula</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>QTable</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># step to next state</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span> <span class=o>=</span> <span class=n>next_state</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>done</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Episode finished after </span><span class=si>{}</span><span class=s1> timesteps, total rewards </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>t</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>rewards</span><span class=p>))</span>
</span></span><span class=line><span class=cl>      <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>env</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span></code></pre></div><h2 class="relative group">Deep Q-Learning<div id=deep-q-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#deep-q-learning aria-label=Anchor>#</a></span></h2><p>在 CartPole 的 task 中，state 只有 4 個 features，action 只有 0 和 1 兩個值。</p><p>但若 state 今天是來自整個環境（例如：整個螢幕、Alpha Go 圍棋棋盤），因為 states
過多，這時用 table 來表示就不太妥當。</p><p>Deep Q-Leanring（以下簡稱 DQN）：我們可以用 deep neural network 幫我們提取
features 並逼近 Q function。</p><p>NN 藉由大量的 input-output end-to-end 訓練，找出：</p><p>$$f(input) = output.$$</p><p>把它轉成 policy：</p><p>$$\pi(state) = action.$$</p><h3 class="relative group">Deep Q-Learning Implementation<div id=deep-q-learning-implementation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#deep-q-learning-implementation aria-label=Anchor>#</a></span></h3><p>在
<a href=https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf target=_blank>Human-level control through deep reinforcement learning</a>
中，提供了三種訓練 DQN 的 tips：</p><ol><li>Use experience replay：把 experience 存在 memory，訓練時隨機抽樣。優點是可以
打亂不同 experience 之間不存在的時間關係。</li><li>Freeze target Q-network：即建立兩種 Q-network，<ul><li>實際進行訓練的 evaluation network</li><li>訓練目標 target network 若只訓練一個 nn，每更新一次時，不但正在訓練的
$Q(s, a)$ 在變，目標 $Q(s&rsquo;, a&rsquo;)$ 也在變，這樣是無法收斂的！</li></ul></li><li>Clip rewards：限縮 reward 的值，以利 backpropagation 中能穩定地計算
gradient。</li></ol><h4 class="relative group">Step 1: 建立 Network<div id=step-1-%E5%BB%BA%E7%AB%8B-network class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-1-%E5%BB%BA%E7%AB%8B-network aria-label=Anchor>#</a></span></h4><p>先建立一層 hidden layer，把 state 傳入後，得出每個 action 的分數，分數越高的
action 越容易被挑選。</p><p>目標：對未來越有利的 action 分數越高。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Net</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_states</span><span class=p>,</span> <span class=n>n_actions</span><span class=p>,</span> <span class=n>n_hidden</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>(</span><span class=n>Net</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>n_states</span><span class=p>,</span> <span class=n>n_hidden</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>n_hidden</span><span class=p>,</span> <span class=n>n_actions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>out</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><h4 class="relative group">Step 2: 建立 Deep Q-Network<div id=step-2-%E5%BB%BA%E7%AB%8B-deep-q-network class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-2-%E5%BB%BA%E7%AB%8B-deep-q-network aria-label=Anchor>#</a></span></h4><p>Tips 中提到，總共需要兩個 network：</p><ul><li>evaluation network（<code>eval_net</code>）</li><li>target network（<code>targ_net</code>）</li></ul><p>和</p><ul><li>memory 儲存 experience</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Deep Q-Network, composed of one eval network, one target network</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DQN</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_states</span><span class=p>,</span> <span class=n>n_actions</span><span class=p>,</span> <span class=n>n_hidden</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>lr</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>,</span> <span class=n>gamma</span><span class=p>,</span> <span class=n>target_replace_iter</span><span class=p>,</span> <span class=n>memory_capacity</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>eval_net</span> <span class=o>=</span> <span class=n>Net</span><span class=p>(</span><span class=n>n_states</span><span class=p>,</span> <span class=n>n_actions</span><span class=p>,</span> <span class=n>n_hidden</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>target_net</span> <span class=o>=</span> <span class=n>Net</span><span class=p>(</span><span class=n>n_states</span><span class=p>,</span> <span class=n>n_actions</span><span class=p>,</span> <span class=n>n_hidden</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># initialize memory, each memory slot is of size (state + next state + reward + action)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>memory</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>memory_capacity</span><span class=p>,</span> <span class=n>n_states</span> <span class=o>*</span> <span class=mi>2</span> <span class=o>+</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>eval_net</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>loss_func</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>memory_counter</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>learn_step_counter</span> <span class=o>=</span> <span class=mi>0</span>  <span class=c1># for target network update</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>n_states</span> <span class=o>=</span> <span class=n>n_states</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>n_actions</span> <span class=o>=</span> <span class=n>n_actions</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>n_hidden</span> <span class=o>=</span> <span class=n>n_hidden</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>batch_size</span> <span class=o>=</span> <span class=n>batch_size</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>lr</span> <span class=o>=</span> <span class=n>lr</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>epsilon</span> <span class=o>=</span> <span class=n>epsilon</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=n>gamma</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>target_replace_iter</span> <span class=o>=</span> <span class=n>target_replace_iter</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>memory_capacity</span> <span class=o>=</span> <span class=n>memory_capacity</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>choose_action</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>state</span><span class=p>),</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># epsilon-greedy</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>()</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>epsilon</span><span class=p>:</span>  <span class=c1># random</span>
</span></span><span class=line><span class=cl>      <span class=n>action</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_actions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># greedy</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=c1># feed into eval net, get scores for each action</span>
</span></span><span class=line><span class=cl>      <span class=n>actions_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>eval_net</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=c1># choose the one with the largest score</span>
</span></span><span class=line><span class=cl>      <span class=n>action</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>actions_value</span><span class=p>,</span> <span class=mi>1</span><span class=p>)[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>numpy</span><span class=p>()[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>store_transition</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>next_state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Pack the experience</span>
</span></span><span class=line><span class=cl>    <span class=n>transition</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>((</span><span class=n>state</span><span class=p>,</span> <span class=p>[</span><span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>],</span> <span class=n>next_state</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Replace the old memory with new memory</span>
</span></span><span class=line><span class=cl>    <span class=n>index</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>memory_counter</span> <span class=o>%</span> <span class=bp>self</span><span class=o>.</span><span class=n>memory_capacity</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>memory</span><span class=p>[</span><span class=n>index</span><span class=p>,</span> <span class=p>:]</span> <span class=o>=</span> <span class=n>transition</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>memory_counter</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>learn</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Randomly select a batch of memory to learn from</span>
</span></span><span class=line><span class=cl>    <span class=n>sample_index</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>memory_capacity</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>b_memory</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>memory</span><span class=p>[</span><span class=n>sample_index</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>    <span class=n>b_state</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>b_memory</span><span class=p>[:,</span> <span class=p>:</span><span class=bp>self</span><span class=o>.</span><span class=n>n_states</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>b_action</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>b_memory</span><span class=p>[:,</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_states</span><span class=p>:</span><span class=bp>self</span><span class=o>.</span><span class=n>n_states</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>b_reward</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>b_memory</span><span class=p>[:,</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_states</span><span class=o>+</span><span class=mi>1</span><span class=p>:</span><span class=bp>self</span><span class=o>.</span><span class=n>n_states</span><span class=o>+</span><span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>b_next_state</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>b_memory</span><span class=p>[:,</span> <span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>n_states</span><span class=p>:])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Compute loss between Q values of eval net &amp; target net</span>
</span></span><span class=line><span class=cl>    <span class=c1># evaluate the Q values of the experiences, given the states &amp; actions taken at that time</span>
</span></span><span class=line><span class=cl>    <span class=n>q_eval</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>eval_net</span><span class=p>(</span><span class=n>b_state</span><span class=p>)</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>b_action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># detach from graph, don&#39;t backpropagate</span>
</span></span><span class=line><span class=cl>    <span class=n>q_next</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>target_net</span><span class=p>(</span><span class=n>b_next_state</span><span class=p>)</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>q_target</span> <span class=o>=</span> <span class=n>b_reward</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> \
</span></span><span class=line><span class=cl>        <span class=n>q_next</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=mi>1</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>batch_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                              <span class=mi>1</span><span class=p>)</span>  <span class=c1># compute the target Q values</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_func</span><span class=p>(</span><span class=n>q_eval</span><span class=p>,</span> <span class=n>q_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Backpropagation</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Update target network every few iterations (target_replace_iter),</span>
</span></span><span class=line><span class=cl>    <span class=c1># i.e. replace target net with eval net</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>learn_step_counter</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>learn_step_counter</span> <span class=o>%</span> <span class=bp>self</span><span class=o>.</span><span class=n>target_replace_iter</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>target_net</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>eval_net</span><span class=o>.</span><span class=n>state_dict</span><span class=p>())</span>
</span></span></code></pre></div><h4 class="relative group">Step 3: 訓練<div id=step-3-%E8%A8%93%E7%B7%B4 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-3-%E8%A8%93%E7%B7%B4 aria-label=Anchor>#</a></span></h4><p>步驟如下：</p><ol><li>選擇 action</li><li>儲存 experience</li><li>train</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>env</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>make</span><span class=p>(</span><span class=s1>&#39;CartPole-v0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>env</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>unwrapped</span>  <span class=c1># For cheating mode to access values hidden in the environment</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Environment parameters</span>
</span></span><span class=line><span class=cl>  <span class=n>n_actions</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=o>.</span><span class=n>n</span>
</span></span><span class=line><span class=cl>  <span class=n>n_states</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>observation_space</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Hyper parameters</span>
</span></span><span class=line><span class=cl>  <span class=n>n_hidden</span> <span class=o>=</span> <span class=mi>50</span>
</span></span><span class=line><span class=cl>  <span class=n>batch_size</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl>  <span class=n>lr</span> <span class=o>=</span> <span class=mf>0.01</span>                 <span class=c1># learning rate</span>
</span></span><span class=line><span class=cl>  <span class=n>epsilon</span> <span class=o>=</span> <span class=mf>0.1</span>             <span class=c1># epsilon-greedy, factor to explore randomly</span>
</span></span><span class=line><span class=cl>  <span class=n>gamma</span> <span class=o>=</span> <span class=mf>0.9</span>               <span class=c1># reward discount factor</span>
</span></span><span class=line><span class=cl>  <span class=n>target_replace_iter</span> <span class=o>=</span> <span class=mi>100</span>  <span class=c1># target network update frequency</span>
</span></span><span class=line><span class=cl>  <span class=n>memory_capacity</span> <span class=o>=</span> <span class=mi>2000</span>
</span></span><span class=line><span class=cl>  <span class=n>n_episodes</span> <span class=o>=</span> <span class=mi>400</span> <span class=k>if</span> <span class=n>CHEAT</span> <span class=k>else</span> <span class=mi>4000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Create DQN</span>
</span></span><span class=line><span class=cl>  <span class=n>dqn</span> <span class=o>=</span> <span class=n>DQN</span><span class=p>(</span><span class=n>n_states</span><span class=p>,</span> <span class=n>n_actions</span><span class=p>,</span> <span class=n>n_hidden</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>lr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>epsilon</span><span class=p>,</span> <span class=n>gamma</span><span class=p>,</span> <span class=n>target_replace_iter</span><span class=p>,</span> <span class=n>memory_capacity</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Collect experience</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>i_episode</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_episodes</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>t</span> <span class=o>=</span> <span class=mi>0</span>  <span class=c1># timestep</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards</span> <span class=o>=</span> <span class=mi>0</span>  <span class=c1># accumulate rewards for each episode</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>  <span class=c1># reset environment to initial state for each episode</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1># Agent takes action</span>
</span></span><span class=line><span class=cl>      <span class=n>action</span> <span class=o>=</span> <span class=n>dqn</span><span class=o>.</span><span class=n>choose_action</span><span class=p>(</span><span class=n>state</span><span class=p>)</span>  <span class=c1># choose an action based on DQN</span>
</span></span><span class=line><span class=cl>      <span class=n>next_state</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span>
</span></span><span class=line><span class=cl>          <span class=n>action</span><span class=p>)</span>  <span class=c1># do the action, get the reward</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1># Cheating part: modify the reward to speed up training process</span>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=n>CHEAT</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>,</span> <span class=n>v</span><span class=p>,</span> <span class=n>theta</span><span class=p>,</span> <span class=n>omega</span> <span class=o>=</span> <span class=n>next_state</span>
</span></span><span class=line><span class=cl>        <span class=c1># reward 1: the closer the cart is to the center, the better</span>
</span></span><span class=line><span class=cl>        <span class=n>r1</span> <span class=o>=</span> <span class=p>(</span><span class=n>env</span><span class=o>.</span><span class=n>x_threshold</span> <span class=o>-</span> <span class=nb>abs</span><span class=p>(</span><span class=n>x</span><span class=p>))</span> <span class=o>/</span> <span class=n>env</span><span class=o>.</span><span class=n>x_threshold</span> <span class=o>-</span> <span class=mf>0.8</span>
</span></span><span class=line><span class=cl>        <span class=c1># reward 2: the closer the pole is to the center, the better</span>
</span></span><span class=line><span class=cl>        <span class=n>r2</span> <span class=o>=</span> <span class=p>(</span><span class=n>env</span><span class=o>.</span><span class=n>theta_threshold_radians</span> <span class=o>-</span> <span class=nb>abs</span><span class=p>(</span><span class=n>theta</span><span class=p>))</span> <span class=o>/</span> \
</span></span><span class=line><span class=cl>            <span class=n>env</span><span class=o>.</span><span class=n>theta_threshold_radians</span> <span class=o>-</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>        <span class=n>reward</span> <span class=o>=</span> <span class=n>r1</span> <span class=o>+</span> <span class=n>r2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1># Keep the experience in memory</span>
</span></span><span class=line><span class=cl>      <span class=n>dqn</span><span class=o>.</span><span class=n>store_transition</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>next_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1># Accumulate reward</span>
</span></span><span class=line><span class=cl>      <span class=n>rewards</span> <span class=o>+=</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1># If enough memory stored, agent learns from them via Q-learning</span>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=n>dqn</span><span class=o>.</span><span class=n>memory_counter</span> <span class=o>&gt;</span> <span class=n>memory_capacity</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dqn</span><span class=o>.</span><span class=n>learn</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1># Transition to next state</span>
</span></span><span class=line><span class=cl>      <span class=n>state</span> <span class=o>=</span> <span class=n>next_state</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=n>done</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Episode finished after </span><span class=si>{}</span><span class=s1> timesteps, total rewards </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>t</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>rewards</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=n>t</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>env</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span></code></pre></div></div></div><script>var oid="views_posts/tech/rl-on-openai-gym.md",oid_likes="likes_posts/tech/rl-on-openai-gym.md"</script><script type=text/javascript src=/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/posts/paper/paper-vid2vid/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">論文筆記 Video-to-Video Synthesis</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2018-11-26T00:00:00+00:00>26 November 2018</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/posts/paper/paper-exp-replay/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">論文筆記 Prioritized Experience Reply</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2018-12-13T00:00:00+00:00>13 December 2018</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Peng-Yu Chen</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://blog.pengyuc.com/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>