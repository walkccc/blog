<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>論文筆記 OctNet: Learning Deep 3D Representations at High Resolutions &#183; Peng-Yu's Blog</title>
<meta name=title content="論文筆記 OctNet: Learning Deep 3D Representations at High Resolutions &#183; Peng-Yu's Blog"><meta name=keywords content="CVPR,paper,"><link rel=canonical href=https://blog.pengyuc.com/posts/paper/paper-octnet/><link type=text/css rel=stylesheet href=/css/main.bundle.min.36c3cd7950e4533fa7da3150d972e3edf34d07f83c0264ff04cad0969dfdb3b8a7065b0ed6c730c6d34a7bad516cfc6f6a5917ab1fdb10b25f481f8a17b54c16.css integrity="sha512-NsPNeVDkUz+n2jFQ2XLj7fNNB/g8AmT/BMrQlp39s7inBlsO1scwxtNKe61RbPxvalkXqx/bELJfSB+KF7VMFg=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.3ad74b33440334ca29f8801ec6dac1f314951cf580e503d49631816b48d1f28d03543275f92b651565b48045b90731f47595b1396214b8b6aa11c6990f867f7e.js integrity="sha512-OtdLM0QDNMop+IAextrB8xSVHPWA5QPUljGBa0jR8o0DVDJ1+StlFWW0gEW5BzH0dZWxOWIUuLaqEcaZD4Z/fg==" data-copy=Copy data-copied=Copied></script><script src=/lib/zoom/zoom.min.f592a181a15d2a5b042daa7f746c3721acf9063f8b6acd175d989129865a37d400ae0e85b640f9ad42cd98d1f8ad30931718cf8811abdcc5fcb264400d1a2b0c.js integrity="sha512-9ZKhgaFdKlsELap/dGw3Iaz5Bj+Las0XXZiRKYZaN9QArg6FtkD5rULNmNH4rTCTFxjPiBGr3MX8smRADRorDA=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://blog.pengyuc.com/posts/paper/paper-octnet/"><meta property="og:site_name" content="Peng-Yu's Blog"><meta property="og:title" content="論文筆記 OctNet: Learning Deep 3D Representations at High Resolutions"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-10-05T00:00:00+00:00"><meta property="article:modified_time" content="2018-10-05T00:00:00+00:00"><meta property="article:tag" content="CVPR"><meta property="article:tag" content="Paper"><meta name=twitter:card content="summary"><meta name=twitter:title content="論文筆記 OctNet: Learning Deep 3D Representations at High Resolutions"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"論文筆記 OctNet: Learning Deep 3D Representations at High Resolutions","headline":"論文筆記 OctNet: Learning Deep 3D Representations at High Resolutions","inLanguage":"en","url":"https:\/\/blog.pengyuc.com\/posts\/paper\/paper-octnet\/","author":{"@type":"Person","name":"Peng-Yu Chen"},"copyrightYear":"2018","dateCreated":"2018-10-05T00:00:00\u002b00:00","datePublished":"2018-10-05T00:00:00\u002b00:00","dateModified":"2018-10-05T00:00:00\u002b00:00","keywords":["CVPR","paper"],"mainEntityOfPage":"true","wordCount":"1513"}]</script><meta name=author content="Peng-Yu Chen"><link href=https://pengyuc.com rel=me><link href=https://github.com/walkccc rel=me><link href=https://linkedin.com/in/pengyuc rel=me><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><link type=text/css rel=stylesheet href=/lib/katex/katex.min.7e7e35e3ef02b7b437449a44ca3fac62ec1ed39cb8312b680a00fe8ac60badc95b063b694636b8440856f7f5e8c2cc9e6b0efb581179b2656c7e1e97558c7096.css integrity="sha512-fn414+8Ct7Q3RJpEyj+sYuwe05y4MStoCgD+isYLrclbBjtpRja4RAhW9/Xowsyeaw77WBF5smVsfh6XVYxwlg=="><script defer src=/lib/katex/katex.min.cadd45c1af1f44bdaf196dc9b104f1daeb29043f0dc59155ffe22847510a04390a0b7a859400d420a626204f7fc5ddb07c19311de1c66b25e19c2559d3e126a8.js integrity="sha512-yt1Fwa8fRL2vGW3JsQTx2uspBD8NxZFV/+IoR1EKBDkKC3qFlADUIKYmIE9/xd2wfBkxHeHGayXhnCVZ0+EmqA=="></script><script defer src=/lib/katex/auto-render.min.e9b2833d28623d18c071d78ef13e9c79d695122d296af3dbcee7bf1bf6518b0565bab59939267fbc8f5faf696193c20f5caef3e7501969cfb306f6738032730d.js integrity="sha512-6bKDPShiPRjAcdeO8T6cedaVEi0pavPbzue/G/ZRiwVlurWZOSZ/vI9fr2lhk8IPXK7z51AZac+zBvZzgDJzDQ==" onload=renderMathInElement(document.body)></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Peng-Yu&rsquo;s Blog</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Posts>Posts</p></a><a href=https://pengyuc.com target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Links
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=https://walkccc.me/LeetCode/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>LeetCode Solutions</p></a><a href=https://www.pokemontcgp.ai target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>PokémonTCGP.ai</p></a></div></div></div></div><a href=https://github.com/walkccc target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Posts>Posts</p></a></li><li class=mt-1><a href=https://pengyuc.com target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Links</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=https://walkccc.me/LeetCode/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>LeetCode Solutions</p></a></li><li class=mt-1><a href=https://www.pokemontcgp.ai target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>PokémonTCGP.ai</p></a></li><li class=mb-2></li><li class=mt-1><a href=https://github.com/walkccc target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul><hr><ul class="flex mt-4 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li class=mb-1><a href=/tags/ class="flex items-center"><p class="text-sm font-sm text-gray-500 hover:text-gray-900" title=Tags>Tags</p></a></li><li class=mb-1><a href=/categories/ class="flex items-center"><p class="text-sm font-sm text-gray-500 hover:text-gray-900" title=Categories>Categories</p></a></li></ul></div></label></div></div><div class="main-menu flex pb-3 flex-col items-end justify-between md:justify-start space-x-3"><div class="hidden md:flex items-center space-x-5"><a href=/tags/ class="flex items-center"><p class="text-xs font-light text-gray-500 hover:text-gray-900" title=Tags>Tags</p></a><a href=/categories/ class="flex items-center"><p class="text-xs font-light text-gray-500 hover:text-gray-900" title=Categories>Categories</p></a></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class=hidden><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/>Peng-Yu's Blog</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/>Posts</a><span class="px-1 text-primary-500">/</span></li><li class=hidden><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/paper/paper-octnet/>論文筆記 OctNet: Learning Deep 3D Representations at High Resolutions</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">論文筆記 OctNet: Learning Deep 3D Representations at High Resolutions</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2018-10-05T00:00:00+00:00>5 October 2018</time><span class="px-2 text-primary-500">&#183;</span><span>1513 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">8 mins</span><span class="px-2 text-primary-500">&#183;</span>
<script type=text/javascript src=/js/zen-mode.min.eea5245cf9244ecbdf2c150d1c8833226c1541cadf6e98f63a7c9192b1a3676df2c3ec603b14f4cfaaa53971fd9d8955640c0f405bf3de2b43ee7a5fb29ae721.js integrity="sha512-7qUkXPkkTsvfLBUNHIgzImwVQcrfbpj2OnyRkrGjZ23yw+xgOxT0z6qlOXH9nYlVZAwPQFvz3itD7npfsprnIQ=="></script><span class=mb-[2px]><span id=zen-mode-button class="text-lg hover:text-primary-500" title="Enable zen mode" data-title-i18n-disable="Enable zen mode" data-title-i18n-enable="Disable zen mode"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 50 50" width="50" height="50"><path fill="currentcolor" d="M12.980469 4C9.1204688 4 5.9804688 7.14 5.9804688 11L6 26H9.9804688V11c0-1.65 1.3400002-3 3.0000002-3H40.019531c1.66.0 3 1.35 3 3V39c0 1.65-1.34 3-3 3H29c0 1.54-.579062 2.94-1.539062 4H40.019531c3.86.0 7-3.14 7-7V11c0-3.86-3.14-7-7-7H12.980469zM7 28c-2.206.0-4 1.794-4 4V42c0 2.206 1.794 4 4 4H23c2.206.0 4-1.794 4-4V32c0-2.206-1.794-4-4-4H7zm0 4H23L23.001953 42H7V32z"/></svg></span></span></span></span></div><div class="flex flex-row flex-wrap items-center"></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/categories/paper/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Paper
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/categories/chinese/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Chinese
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/cvpr/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">CVPR
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='return window.open("/tags/paper/","_self"),!1'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Paper</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Peng-Yu Chen" src=/img/blowfish_logo_hu_e74a130226122ae3.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Peng-Yu Chen</div><div class="text-sm text-neutral-700 dark:text-neutral-400">A little bit about you</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=mailto:me@pengyuc.com target=_blank aria-label=Email rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://pengyuc.com target=_blank aria-label=Link rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/walkccc target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://linkedin.com/in/pengyuc target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#sparse-models>Sparse Models</a></li></ul><ul><li><a href=#31-hybrid-grid-octree-data-structure>3.1. Hybrid Grid-Octree Data Structure</a></li><li><a href=#32-networks-operations>3.2. Networks Operations</a><ul><li><a href=#convolution>Convolution</a></li><li><a href=#pooling>Pooling</a></li><li><a href=#unpooling>Unpooling</a></li></ul></li></ul><ul><li><a href=#41-3d-classification>4.1. 3D Classification</a></li><li><a href=#42-3d-orientation-estimation>4.2. 3D Orientation Estimation</a></li><li><a href=#43-3d-semantic-segmentation>4.3. 3D Semantic Segmentation</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#sparse-models>Sparse Models</a></li></ul><ul><li><a href=#31-hybrid-grid-octree-data-structure>3.1. Hybrid Grid-Octree Data Structure</a></li><li><a href=#32-networks-operations>3.2. Networks Operations</a><ul><li><a href=#convolution>Convolution</a></li><li><a href=#pooling>Pooling</a></li><li><a href=#unpooling>Unpooling</a></li></ul></li></ul><ul><li><a href=#41-3d-classification>4.1. 3D Classification</a></li><li><a href=#42-3d-orientation-estimation>4.2. 3D Orientation Estimation</a></li><li><a href=#43-3d-semantic-segmentation>4.3. 3D Semantic Segmentation</a></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})(),function(){var t,e=$("#TableOfContents");if(e.length>0){t=$(window);function n(){var s,o=t.scrollTop(),i=$(".anchor"),n="";if(i.each(function(e,t){t=$(t),t.offset().top-$(window).height()/3<=o&&(n=decodeURIComponent(t.attr("id")))}),s=e.find("a.active"),s.length==1&&s.eq(0).attr("href")=="#"+n)return!0;s.each(function(e,t){$(t).removeClass("active").siblings("ul").hide()}),e.find('a[href="#'+n+'"]').addClass("active"),e.find('a[href="#'+n+'"]').parentsUntil("#TableOfContents").each(function(e,t){$(t).children("a").parents("ul").show()})}t.on("scroll",n),$(document).ready(function(){e.find("a").parent("li").find("ul").hide(),n()})}}()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><p><a href=http://openaccess.thecvf.com/content_cvpr_2017/papers/Riegler_OctNet_Learning_Deep_CVPR_2017_paper.pdf target=_blank>Paper Link</a></p><p>此篇 paper 的主要目的是要解決，但送進網路 training 的 3D 資料是稀疏的（sparse）
，用原本的描述方式會太耗費記憶體而難以實作，因為實際上很多地方都是沒有值的，本篇
paper 重點是：使用 octree 的方法來表示高解析度的「立體圖像」，卻又不失本來「完整
地」表示方式。</p><h1 class="relative group">1. Introduction<div id=1-introduction class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#1-introduction aria-label=Anchor>#</a></span></h1><p>現有的 3D 網路架構，大多會用像類似 2D pixels 的表示方示，在 2D 我們稱之為
pixel，在 3D 的話就稱之為 voxel。同理，在 2D 世界的 convolution kernel 在 3D 就
會變成一個長方體，大小可能是 \(3 \times 3 \times 3\) 之類。</p><p>引述原 paper 中的描述：</p><blockquote><p>a dense and regular 3D voxel grid, and process this grid using 3D convolution
and pooling operations.</p></blockquote><p>並且現今的 3D networks 受限於記憶體大小，數量級數通常大約在 \(30^3\) voxels。</p><p>從下圖中可以看到，OctNet 可以用「明顯較少」的位元數去表示到一模一樣的 notation（
圖中每 1 個小正方型耗費的記憶體為 1 單位）</p><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/4CBOTnU.png alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>Figure 1: <strong>Motivation.</strong></td></tr></tbody></table><p>大部分 3D 資料常常是稀疏的，例如：</p><ul><li>point clouds</li><li>meshes</li></ul><p>所以其實在大多時間，3D convolution 的計算都浪費了不少資源。並且真的在計算時
，high activations 大多都是發在在邊界情況（因為很多地方沒值啊！）</p><h1 class="relative group">2. Related Work<div id=2-related-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#2-related-work aria-label=Anchor>#</a></span></h1><h2 class="relative group">Sparse Models<div id=sparse-models class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#sparse-models aria-label=Anchor>#</a></span></h2><ul><li>Engelcke：透過將值推送到其目標位置來計算稀疏輸入位置處的卷積<ul><li>pros：降低所需要的計算次數</li><li>cons：記憶體資源還是非常耗</li></ul></li><li>reducing sparse convolutions to matrix operations<ul><li>cons：只允許 \(2 \times 2\) 的卷積，而且需來來回 indexing 拷貝，所以仍有
overhead，最高解析度大概只能做到 \(80^3\) voxels。</li></ul></li><li>Li：field probing networks (FBL)：先對 3D data 進行 sample，再餵進網路<ul><li>pros：結省記憶體、計算量</li><li>cons：因為 FBL 被，stacked、convolved、pooled，所以無法完美發揮 ConvNets 的
力量。</li></ul></li></ul><h1 class="relative group">3. Octree Networks<div id=3-octree-networks class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#3-octree-networks aria-label=Anchor>#</a></span></h1><p>為了減少表示 sparse 3D data 所需要的記憶體空間，paper 提出了一種針對整個立方體的
劃分方法：將計算集中在<strong>相關區域</strong>上。而
<a href=https://zh.wikipedia.org/zh-tw/%e5%85%ab%e5%8f%89%e6%a8%b9 target=_blank>octrees</a> 就是一個很棒的劃分 3D voxel 的
表示法。</p><p>一般來說，octrees 會使用 pointers 來實作，如此就能達到「真實」地降低記憶體需求量
，但 ConvNet 的 operation 通常需要頻繁地訪問鄰居的值，因此太深的 octrees 會讓訪
問時間過大，為了解決這個問題，本篇 paper 提出了下節的 hybrid grid octree data
structure。</p><h2 class="relative group">3.1. Hybrid Grid-Octree Data Structure<div id=31-hybrid-grid-octree-data-structure class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#31-hybrid-grid-octree-data-structure aria-label=Anchor>#</a></span></h2><p>為了解決上述所提出的<strong>深度</strong>問題（深度會增加訪問的時間複雜度），此篇 paper 提出
了「限制 octree 最大深度」的想法。</p><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/nZ5TIlo.png alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>Figure 2: <strong>Hybrid Grid-Octree Data Structure.</strong> This example illustrates a hybrid grid-octree consisting of $8$ shallow octrees indicated by different colors. Using $2$ shallow octrees in each dimension with a maximum depth of $3$ leads to a total resolution of $16^3$ voxels.</td></tr></tbody></table><p>Fig. 2 中將最大深度限制為 \(3\)，所以本來 \(1\) 棵標準的八叉樹就需要用
\(8\) 棵淺八叉樹（shallow octrees，以下簡稱淺樹）來表示，雖然這個資料結構不會
像標準的八叉樹那麼省記憶體，因為我們需要更多的淺樹來表示，但是我們成功的將本來要
\(\log_2 16 = 4\) 層的 pointers 降到了 \(\log_2 8 = 3\) 層的 pointers，而且
記憶體還是耗費在一個 \(O(1)\) 之內，並且還能再做壓縮。</p><p>例如像若有一棵淺樹沒有任何 input data，他就只要用一個 \(\textbf 0\) 向量來表
示本來 \(8^3 = 512\) 個 \(\textbf 0\) 向量（假設深度一樣是 \(3\)）！</p><p>八叉樹的一個大優點就是可以有效率地編碼成 bit string representation，不僅有效降低
訪問次數，也可運用 <a href=https://zh.wikipedia.org/zh-tw/%e5%9c%96%e5%bd%a2%e8%99%95%e7%90%86%e5%99%a8%e9%80%9a%e7%94%a8%e8%a8%88%e7%ae%97 target=_blank>GPGPU</a> 有
效地實作。</p><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/kO9G7NB.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/MlNkxcU.png alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>3(a) Shallow Octree</td><td style=text-align:center>3(b) Bit-Representation</td></tr><tr><td style=text-align:center>Figure 3: <strong>Bit Representation.</strong> Shallow octrees can be efficiently encoded using bit-strings.</td><td style=text-align:center>the bit-string 1 01010000 00000000 01010000 00000000 010100000&mldr; defines the octree in (a)</td></tr></tbody></table><p>使用這種 bit-representation，淺八叉樹中的單個 voxel 完全由其 bit index 表示。此
index 確定：</p><ul><li>voxel 的深度</li><li>voxel 的大小（越深越小）</li></ul><p>可以使用簡單的算法來檢索 index \(i\) 的 voxel 的相應的父、子索引，而不是使用指
向父節點和子節點的 pointer：</p><p>$$
\text{pa}(i) = \Bigg \lfloor \frac{i - 1}{8} \Bigg \rfloor, \tag{1}
$$</p><p>$$
\text{ch}(i) = 8 \cdot i + 1. \tag{2}
$$</p><p>我們將淺樹的 data (storgin features vectors) 放在一個連續的陣列，因此我們要能快
速的計算：當給定一個 bit-representation 中的 index \(i\) 時，算出他對應的 data
index，公式如下：</p><p>$$
\text{data\_idx}(i) = \underbrace{8 \sum_{j = 0}^{\text{pa}(i) - 1} \text{bit}(j) + 1}_{\#\text{nodes above }i} - \underbrace{\sum_{j = 0}^{i - 1} \text{bit}(j)}_{\#\text{split nodes pre }i} + \underbrace{\text{mod}(i - 1, 8)}_{\text{offset}}. \tag{3}
$$</p><p>看起來很複雜，對吧？</p><blockquote><p>引述 A.3. Efficient Convolution 的說明：</p></blockquote><p>paper 有提供了 Appendix 的例子供參考，這裡做個筆記，為了能夠圖像化，將八叉樹改為
四叉樹，對應的公式修正如下：</p><p>$$
\text{data\_idx}_4(i) =
\underbrace{4 \sum_{j = 0}^{\text{pa}(i) - 1} \text{bit}(j) + 1}_{\#\text{nodes above }i} -
\underbrace{\sum_{j = 0}^{i - 1} \text{bit}(j)}_{\#\text{split nodes pre }i} +
\underbrace{\text{mod}(i - 1, 4)}_{\text{offset}}. \tag{14}
$$</p><p>和</p><p>$$
\text{pa}_4(i) = \Bigg \lfloor \frac{i - 1}{4} \Bigg \rfloor.
$$</p><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/QXqBTpj.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/Z0zEVAT.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/ZWXYU3u.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/V2q3xb4.png alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>13(a) Bit-String</td><td style=text-align:center>13(b) Split and Leaf Nodes</td><td style=text-align:center>13(c) Bit Index</td><td style=text-align:center>13(d) Data Index</td></tr></tbody></table><p>例如我們要找的 bit，index 為 \(51\)，我們首先根據找 parent 的公式：</p><p>$$
\text{pa}_4(51) = \Bigg \lfloor \frac{51 - 1}{4} \Bigg \rfloor = 12.
$$</p><p>得到 parent bit index 為 \(12\)，再來計算 parent \(12\) 的第一個 child：</p><p>$$
12 \cdot 4 + 1 = 49.
$$</p><p>再根據式 (14) 第一項：找出由 bit \(0\) 到 bit \(11\) 的 bit value 是 \(1\)
的數量（即：有叉開處），以這裡來說共有 \(4\) 個，分別在 bit index \(0\),
\(2\), \(4\) 和 \(9\)，所以得到在 bit \(49\) 以前的點共有 \(4 \cdot 4 +
1 = 17\) 個</p><p>式 (14) 第二項：在 bit \(51\) 前 bit value 是 \(1\) 的數量，以這裡來說共有
\(6\) 個，分別在 bit index \(0\), \(2\), \(4\), \(9\), \(12\) 和
\(18\)，所以這些待會要扣掉 \(6\)</p><p>式 (14) 第三項單純的找出 offset 為</p><p>$$
\text{mod}(51 - 1, 4) = 2.
$$</p><p>最後就能求得 bit $51$ 的 data index 為 $17 - 6 + 2 = 13$。</p><h2 class="relative group">3.2. Networks Operations<div id=32-networks-operations class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#32-networks-operations aria-label=Anchor>#</a></span></h2><p>來講講 notation 的表示方法：</p><ul><li><p>\(T_{i, j, k}\)：在位置 \((i, j, k)\) 的 3D tensor \(T\)</p></li><li><p>給定 hybrid grid-octree structure 和大小為 \(D \times H \times W\) 的淺樹，
並且深度 \(\le 3\)</p></li><li><p>\(O[i, j, k]\)：所有能夠包住 voxel \((i, j, k)\) 中，最小 cell 的 value</p><p>所以即便 \(i_1 \ne i_2 \lor j_1 \ne j_2 \lor k_1 \ne k_2\)，\(O[i_1, j_1,
k_1]\) 和 \(O[i_2, j_2, k_2]\) 仍有可能對應到同一個 hybrid grid-octree
中的 voxel，這是由 voxels 的大小所決定的。</p></li></ul><p>另外我們可以得到淺樹的 index 為 \((\big\lfloor \frac i 8 \big\rfloor,
\big\lfloor \frac j 8 \big\rfloor, \big\lfloor \frac k 8 \big\rfloor)\)，且對其
中一個淺樹的 local index 為 \((\text{mod}(i, 8), \text{mod}(j, 8),
\text{mod}(k, 8))\)。</p><p>有了上述的 notation 後，我們可以得到由 grid-octree \(O\) 到 tensor \(T\) 的
mapping 如下：</p><p>$$
\text{oc2ten}: T_{i, j, k} = O[i, j, k]. \tag{4}
$$</p><p>這裡可以理解成：我們要找 \((i, j, k)\) 的值，即：</p><p>\(\forall\) cell \(\in O\)，</p><ul><li>能包住 \((i, j, k)\)</li><li>大小又是最小的 cell</li></ul><p>該 cell 的 value 就是位置 \((i, j, k)\) 的 tensor。</p><p>因此可能會有好幾個不同的位置，因為都在同一個 cell 之中，他們的 value 就一樣。</p><p>同時也可得到逆向的 mapping：</p><p>$$
\text{ten2oc}: O[i, j, k] = \text{pool\_voxels}_{(\bar i, \bar j, \bar k) \in \Omega[i, j, k]} (T_{\bar i, \bar j, \bar k}), \tag{5}
$$</p><p>其中，</p><ul><li>\(\text{pool\_voxels}(\cdot)\)：pooling function (e.g., average or
max-pooling)，對所有在 \(T\) 中的 voxels 做 pooling</li></ul><p>可以理解成：\(\forall (\bar i, \bar j, \bar k)\) 在 \((i, j, k)\) 所對應到的
範圍內（該 cell，最大會是 \(8^3 = 512\)），對他們每個 voxels 所對應的 tensor
\(T_{\bar i, \bar j, \bar k}\) 做 pooling</p><h3 class="relative group">Convolution<div id=convolution class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#convolution aria-label=Anchor>#</a></span></h3><p>Convolution 無疑是最重要也最吃計算資源的，對只有單一 feature map 的每個 voxels
來說，對每一個送進來的 input tensor \(T_{\hat i, \hat j, \hat k}^\text{in}\)
和 3D kernel \(W \in \mathbb R^{L \times M \times N}\) 可以被寫成：</p><p>$$
T_{i, j, k}^\text{out} = \sum_{l = 0}^{L - 1} \sum_{m = 0}^{M - 1} \sum_{n = 0}^{N - 1} W_{l, m, n} \cdot T_{\hat i, \hat j, \hat k}^\text{in}, \tag{7}
$$</p><p>其中：</p><ul><li>\(\hat i = i - l + \lfloor L / 2 \rfloor\)</li><li>\(\hat j = j - m + \lfloor M / 2 \rfloor\)</li><li>\(\hat k = k - n + \lfloor N / 2 \rfloor\)</li></ul><p>同樣的，對 grid-octree 做 convolutions：</p><p>$$
\text{ten2oc}: O^\text{out}[i, j, k] = \text{pool\_voxels}_{(\bar i, \bar j, \bar k) \in \Omega[i, j, k]} (T_{\bar i, \bar j, \bar k}) \tag{8}
$$</p><p>$$
T_{i, j, k} = \sum_{l = 0}^{L - 1} \sum_{m = 0}^{M - 1} \sum_{n = 0}^{N - 1} W_{l, m, n} \cdot O^\text{in}[\hat i, \hat j, \hat k].
$$</p><p>對 \(\forall (i, j, k) \in \text{cell } \Omega[i, j, k]\) 做卷積的話，若
octree cell 大小為 \(8^3\)，kernel 大小為 \(3^3\) 的話，共要做</p><p>\(8^3 \cdot 3^3 = 13,824\) 次乘法。但是，我們可以更有效地計算，如 Fig. 14 的例
子所示：</p><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/p2NMl8b.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/kAC2ood.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/VS85sj4.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/FmPQTYn.png alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>14(a) Constant</td><td style=text-align:center>14(b) Corners</td><td style=text-align:center>14(c) Edges</td><td style=text-align:center>14(d) Faces</td></tr></tbody></table><ul><li><p>Fig. 14a：觀察到大部分中心的值是固定的。因此，我們只需要在中心內一次計算一次卷
積並將結果乘以 \(8^3\)</p></li><li><p>Fig. 14b-d：另外，我們只需要在 voxel 的 cornerr、edges 和 faces 上計算 kernel
的截斷版本。這樣的實作方法很有效率，因為我們總共只需要：</p><ul><li>對中心 constant 部分進行 \(3^3 = 27\) 次乘法</li><li>對 corners 進行 \(8 \cdot 19 = 152\) 次乘法</li><li>對 edges 進行 \(12 \cdot 6 \cdot 15 = 1080\) 次乘法</li><li>對 faces 進行 \(6 \cdot 6^2 \cdot 9 = 1944\) 次乘法</li></ul></li></ul><p>總共產生 \(27 + 152 + 1080 + 1944 = 3203\) 次乘法，是本來 \(13,824\) 的
\(23.17%\)，能有效降低計算的時間複雜度。</p><p>所以可以得到下面的 Fig. 4，有別於傳統的 voxel by voxel convolution，paper 中提出
來：透過分開計算各個 voxel</p><ul><li>constant</li><li>與鄰居的截斷 convolution</li></ul><p>再相加明顯有效率很多。</p><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/JN7LP7m.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/5piURBn.png alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>4(a) Standard Convolution</td><td style=text-align:center>4(b) Efficient Convolution</td></tr></tbody></table><h3 class="relative group">Pooling<div id=pooling class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#pooling aria-label=Anchor>#</a></span></h3><p>Pooling 依序從最小的 cell 開始，每 \(2^3\) 個當前最小的 cell 做一次 pooling，
公式可寫成如下：</p><p>$$
T_{i, j, k}^\text{out} = \max_{l, m, n \in [0, 1]} (T_{2i + l, 2j + m, 2k + n}^\text{in}), \tag{9}
$$</p><p>其中：</p><ul><li>\(T^\text{in} \in \mathbb R^{2D \times 2H \times 2W}\)</li><li>\(T^\text{out} \in \mathbb R^{D \times H \times W}\)</li></ul><p>對一個有 \(2D \times 2H \times 2W\) 棵淺樹的 input grid octree
\(O^\text{in}\) 而言，\(O^\text{out}\) 會有 \(D \times H \times W\) 棵淺樹
，每一個 \(O^\text{in}\) 中的 voxel 都大小都會對半再複製到更深 1 層的淺樹，例
如以深度為 \(3\) 的 \(O^\text{in}\) 做一次 pooling，公式可以寫成：</p><p>$$
O^\text{out}[i, j, k] = \begin{cases}
O^\text{in}[2i, 2j, 2k] & \text{ if vxd}(2i, 2j, 2k) &lt; 3; \\
P & \text{ else}
\end{cases}
$$</p><p>$$
P = \max_{l, m, n \in [0, 1]} (O^\text{in}[2i + l, 2j + m, 2k + n]), \tag{10}
$$</p><p>其中 \(\text{vxd}(\cdot)\) 計算在淺樹中 indexed voxel 的深度。</p><p>透過 Fig. 5 能有更深的了解：</p><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/wIqf6pT.png alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>Figure 5: <strong>Pooling.</strong> The \(2^3\) pooling operation on the grid-octree structure combines \(8\) neighbouring shallow octrees (a) into one shallow octree (b). The size of each voxel is halved and copied to the new shallow octree structure. Voxels on the finest resolution are pooled. Different shallow octrees are depicted in different colors.</td></tr></tbody></table><h3 class="relative group">Unpooling<div id=unpooling class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#unpooling aria-label=Anchor>#</a></span></h3><p>有了 pooling 的概念後，unpooling 就只是逆向回去，也單純很多，寫成公式如下：</p><p>$$
T_{i, j, k}^\text{out} = T_{\lfloor i / 2 \rfloor, \lfloor j / 2 \rfloor, \lfloor k / 2 \rfloor}^\text{in}. \tag{11}
$$</p><p>$$
O^\text{out}[i, j, k] = O^\text{in}[\lfloor i / 2 \rfloor, \lfloor j / 2 \rfloor, \lfloor k / 2 \rfloor]. \tag{12}
$$</p><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/K0WflKD.png alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>Figure 6: <strong>Unpooling.</strong> The \(2^3\) unpooling operation transforms a single shallow octree of depth \(d\) as shown in (a) into \(8\) shallow octrees of depth \(d - 1\), illustrated in (b). For each node at depth zero one shallow octree is spawned. All other voxels double in size. Different shallow octrees are depicted in different colors.</td></tr></tbody></table><h1 class="relative group">4. Experimental Evaluation<div id=4-experimental-evaluation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#4-experimental-evaluation aria-label=Anchor>#</a></span></h1><h2 class="relative group">4.1. 3D Classification<div id=41-3d-classification class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#41-3d-classification aria-label=Anchor>#</a></span></h2><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/G1Q8Etz.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/cWX2YpO.png alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>Figure 7: <strong>Results on ModelNet10 Classification Task.</strong></td><td style=text-align:center>Figure 8: <strong>Voxelized 3D Shapes from ModelNet10.</strong></td></tr></tbody></table><p>從 Fig. 7 中可以看出，OctNet 在各項 metrics 中都有很強大的能力，Fig. 7c 是指每個
block 的卷積層數分別固定在 \(1\)、\(2\) 和 \(3\)。Fig 7d 中，甚至可發現
DenseNet 只能做到 \(64^3\) 的最大解析度，遠遠不及 OctNet 能做到 \(256^3\) 的
解析度。</p><h2 class="relative group">4.2. 3D Orientation Estimation<div id=42-3d-orientation-estimation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#42-3d-orientation-estimation aria-label=Anchor>#</a></span></h2><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/8MgLW0l.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/p36WXc8.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/86NKdlK.png alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>Figure 9: <strong>Confusion Matrices on ModelNet10.</strong></td><td style=text-align:center>Figure 10: <strong>Orientation Estimation on ModelNet10.</strong></td><td style=text-align:center>Figure 11: <strong>Orientation Estimation on ModelNet10.</strong> This figure illustrates $10$ rotation estimates for \(3\) chair instances while varying the input resolution from \(16^3\) to \(128^3\). Darker colors indicate larger deviations from the ground truth.</td></tr></tbody></table><h2 class="relative group">4.3. 3D Semantic Segmentation<div id=43-3d-semantic-segmentation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#43-3d-semantic-segmentation aria-label=Anchor>#</a></span></h2><table><thead><tr><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/D59VRRD.png alt></figure></th><th style=text-align:center><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/nm0vMvV.jpg alt></figure></th></tr></thead><tbody><tr><td style=text-align:center>Table 1: <strong>Semantic Segmentation on RueMonge2014.</strong></td><td style=text-align:center>Figure 12: <strong>OctNet \(256^3\) Facade Labeling Results.</strong></td></tr></tbody></table><h1 class="relative group">5. Conclusion and Future Work<div id=5-conclusion-and-future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#5-conclusion-and-future-work aria-label=Anchor>#</a></span></h1><p>OctNet 是一種新穎的 3D representation，可以讓高解析度輸入深度學習。我們分析了高
解析度輸入對幾個 3D 學習任務的重要性，例如對象分類、姿勢估計和語義分割。我們的實
驗表明，對於 ModelNet10 分類，低解析度網絡證明是足夠的，而高輸入（和輸出）解析度
對於 3D 方向估計和 3D point cloud 標記很重要。但此篇 paper 作者相信，OctNet 能夠
將 3D 的深度學習帶向更高解析度的世界。</p></div></div><script>var oid="views_posts/paper/paper-octnet.md",oid_likes="likes_posts/paper/paper-octnet.md"</script><script type=text/javascript src=/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/posts/paper/paper-pointwise-cnn/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">論文筆記 Pointwise Convolutional Neural Networks</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2018-09-27T00:00:00+00:00>27 September 2018</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/posts/paper/paper-pointnet/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">論文筆記 PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2018-10-09T00:00:00+00:00>9 October 2018</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Peng-Yu Chen</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://blog.pengyuc.com/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>